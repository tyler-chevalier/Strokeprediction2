





import numpy as np
import pandas as pd


# Load the stroke dataset
df = pd.read_csv('healthcare-dataset-stroke-data.csv')

# Remove duplicate rows
df_no_duplicates = df.drop_duplicates()
print(f"Removed {df.shape[0] - df_no_duplicates.shape[0]} duplicate rows")

# Convert 'N/A' to NaN in the bmi column
df_no_duplicates['bmi'] = df_no_duplicates['bmi'].replace('N/A', np.nan).astype(float)

# Count missing values before filling
missing_age = df_no_duplicates['age'].isna().sum()
missing_bmi = df_no_duplicates['bmi'].isna().sum()
missing_glucose = df_no_duplicates['avg_glucose_level'].isna().sum()

# Calculate medians for age and glucose
age_median = df_no_duplicates['age'].median()
glucose_median = df_no_duplicates['avg_glucose_level'].median()

# Calculate gender-specific BMI medians
male_bmi_median = df_no_duplicates[df_no_duplicates['gender'] == 'Male']['bmi'].median()
female_bmi_median = df_no_duplicates[df_no_duplicates['gender'] == 'Female']['bmi'].median()
overall_bmi_median = df_no_duplicates['bmi'].median()  # For any other gender category

# Fill missing values with appropriate medians
df_no_duplicates['age'] = df_no_duplicates['age'].fillna(age_median)
df_no_duplicates['avg_glucose_level'] = df_no_duplicates['avg_glucose_level'].fillna(glucose_median)

# Fill BMI based on gender
df_no_duplicates.loc[(df_no_duplicates['bmi'].isna()) & (df_no_duplicates['gender'] == 'Male'), 'bmi'] = male_bmi_median
df_no_duplicates.loc[(df_no_duplicates['bmi'].isna()) & (df_no_duplicates['gender'] == 'Female'), 'bmi'] = female_bmi_median
df_no_duplicates.loc[(df_no_duplicates['bmi'].isna()), 'bmi'] = overall_bmi_median  # For any remaining NaN values


# Print how many missing values were replaced for each column
print(f"Replaced {missing_age} missing values in Age column")
print(f"Replaced {missing_bmi} missing values in BMI column (Male median: {male_bmi_median:.2f}, Female median: {female_bmi_median:.2f})")
print(f"Replaced {missing_glucose} missing values in Glucose Level column")








## change the gender column to binary values
df_no_duplicates['gender'] = df_no_duplicates['gender'].map({'Female': 0, 'Male': 2, 'Other': 1})

## change the ever_married column to binary values
df_no_duplicates['ever_married'] = df_no_duplicates['ever_married'].map({'No': 0, 'Yes': 1})

##change work_type column to multi class values
df_no_duplicates['work_type'] = df_no_duplicates['work_type'].map({'Govt_job': 0,'Private': 1, 'Self-employed': 2, 'children': 3, 'Never_worked': 4})

## change residence_type column to binary values
df_no_duplicates['Residence_type'] = df_no_duplicates['Residence_type'].map({'Urban': 0, 'Rural': 1})

## change smoking_status column to multi class values
df_no_duplicates['smoking_status'] = df_no_duplicates['smoking_status'].map({'never smoked': 0, 'formerly smoked': 1, 'smokes': 2, 'Unknown': 3})







# Save the processed data to a new CSV file
output_file = 'processed_stroke_data.csv'
df_no_duplicates.to_csv(output_file, index=False)
print(f"Processed data saved to {output_file}")
print(f"Filled missing values with medians - Age: {age_median}, Glucose: {glucose_median}")





from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import class_likelihood_ratios
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt



# Naive Bayes using all features
feature_columns = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status']
X_dataset = pd.read_csv('processed_stroke_data.csv', usecols=feature_columns)
y_columns = ['stroke']
y_dataset = pd.read_csv('processed_stroke_data.csv', usecols=y_columns)

scaler = StandardScaler()
X_dataset_normalized = scaler.fit_transform(X_dataset)

y_columns = ['stroke']
y_dataset = pd.read_csv('processed_stroke_data.csv', usecols=y_columns)

X_train, X_test, y_train, y_test = train_test_split(X_dataset_normalized, y_dataset, test_size=0.7, random_state=0)
gaussian_orig = GaussianNB()

y_pred = gaussian_orig.fit(X_train, y_train).predict(X_test)

y_test_r = np.ravel(y_test)
print("%d mislabeled points out of a total of %d points" % ((y_test_r != y_pred).sum(), X_test.shape[0]))

acc_orig = accuracy_score(y_test, y_pred)
print("Accuracy score: %f" % (acc_orig))

f1_orig = f1_score(y_test, y_pred)
print("F1 score: %f" % (f1_orig))

prec_orig = precision_score(y_test, y_pred)
print("Precision score: %f" % (prec_orig))

rec_orig = recall_score(y_test, y_pred)
print("Recall score: %f" % (rec_orig))

class_orig = class_likelihood_ratios(y_test, y_pred, labels=[0,1])
print("LR+: %f" % (class_orig[0]))
print("LR-: %f" % (class_orig[1]))


# Naive Bayes feature reduced variant 1 (remove features with little effect)
feature_columns = ['age', 'hypertension', 'ever_married', 'work_type', 'avg_glucose_level', 'bmi']
X_dataset = pd.read_csv('processed_stroke_data.csv', usecols=feature_columns)
y_columns = ['stroke']
y_dataset = pd.read_csv('processed_stroke_data.csv', usecols=y_columns)

scaler = StandardScaler()
X_dataset_normalized = scaler.fit_transform(X_dataset)

y_columns = ['stroke']
y_dataset = pd.read_csv('processed_stroke_data.csv', usecols=y_columns)

X_train, X_test, y_train, y_test = train_test_split(X_dataset_normalized, y_dataset, test_size=0.7, random_state=0)

gaussian_orig = GaussianNB()

y_pred = gaussian_orig.fit(X_train, y_train).predict(X_test)

y_test_r = np.ravel(y_test)
print("%d mislabeled points out of a total of %d points" % ((y_test_r != y_pred).sum(), X_test.shape[0]))

acc_var1 = accuracy_score(y_test, y_pred)
print("Accuracy score: %f" % (acc_var1))

f1_var1 = f1_score(y_test, y_pred)
print("F1 score: %f" % (f1_var1))

prec_var1 = precision_score(y_test, y_pred)
print("Precision score: %f" % (prec_var1))

rec_var1 = recall_score(y_test, y_pred)
print("Recall score: %f" % (rec_var1))

class_var1 = class_likelihood_ratios(y_test, y_pred, labels=[0,1])
print("LR+: %f" % (class_var1[0]))
print("LR-: %f" % (class_var1[1]))




# Naive Bayes feature reduced variant 2 (highest accuracy)
feature_columns = ['age', 'hypertension', 'bmi', 'smoking_status']
X_dataset = pd.read_csv('processed_stroke_data.csv', usecols=feature_columns)

scaler = StandardScaler()
X_dataset_normalized = scaler.fit_transform(X_dataset)

y_columns = ['stroke']
y_dataset = pd.read_csv('processed_stroke_data.csv', usecols=y_columns)

X_train, X_test, y_train, y_test = train_test_split(X_dataset_normalized, y_dataset, test_size=0.7, random_state=0)

gaussian_orig = GaussianNB()

y_pred = gaussian_orig.fit(X_train, y_train).predict(X_test)

y_test_r = np.ravel(y_test)
print("%d mislabeled points out of a total of %d points" % ((y_test_r != y_pred).sum(), X_test.shape[0]))

acc_var2 = accuracy_score(y_test, y_pred)
print("Accuracy score: %f" % (acc_var2))

f1_var2 = f1_score(y_test, y_pred)
print("F1 score: %f" % (f1_var2))

prec_var2 = precision_score(y_test, y_pred)
print("Precision score: %f" % (prec_var2))

rec_var2 = recall_score(y_test, y_pred)
print("Recall score: %f" % (rec_var2))

class_var2 = class_likelihood_ratios(y_test, y_pred, labels=[0,1])
print("LR+: %f" % (class_var2[0]))
print("LR-: %f" % (class_var2[1]))


variants = ['Original', 'Variant 1', 'Variant 2']
accuracies = [acc_orig, acc_var1, acc_var2]

plt.bar(variants, accuracies)
plt.title('Naive Bayes Accuracies')
plt.ylabel('Score')
plt.show()


variants = ['Original', 'Variant 1', 'Variant 2']
f1_scores = [f1_orig, f1_var1, f1_var2]

plt.bar(variants, f1_scores)
plt.title('Naive Bayes F1 Scores')
plt.ylabel('Score')
plt.show()


variants = ['Original', 'Variant 1', 'Variant 2']
LRp_scores = [class_orig[0], class_var1[0], class_var2[0]]
LRm_scores = [class_orig[1], class_var1[1], class_var2[1]]

w, x = 0.4, np.arange(len(variants))

fig, ax = plt.subplots()
ax.bar(x-w/2, LRp_scores, width=w, label="LR+ Scores")
ax.bar(x+w/2, LRm_scores, width=w, label="LR- Scores")

ax.set_xticks(x)
ax.set_xticklabels(variants)
ax.set_ylabel("Scores")
ax.set_title("Naive Bayes Class Likelihood Ratios")
ax.legend()

plt.show()






























